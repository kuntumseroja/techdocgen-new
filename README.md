# TechDocGen by IBMC

A comprehensive tool for generating technical documentation from source code in Java, .NET (C#, VB.NET, F#), and PHP. Supports reading from single files, folders, or Git repositories, and uses LLMs (OpenAI, Anthropic, Ollama, or MCP) to generate high-quality documentation.

**TechDocGen by IBMC** - Empowering developers with AI-driven documentation generation.

## Features

- **Multiple Source Types**: Read from single files, folders, or Git repositories
- **Language Support**: Java, .NET (C#, VB.NET, F#), PHP, JavaScript/TypeScript, HTML, and JSON/YAML configs
- **LLM Integration**: Support for multiple LLM providers:
  - OpenAI (GPT-4, GPT-3.5)
  - Anthropic (Claude)
  - Ollama (Local LLMs)
  - MCP (Model Context Protocol)
- **Intelligent Parsing**: Extracts classes, methods, interfaces, imports, types, and more
- **Markdown Output**: Generates well-formatted markdown documentation
- **Large Repo Support**: Streaming output + chunking for huge files
- **Domain Profiles**: Per-service documentation with scoped include/exclude rules
- **Messaging Flow Extraction**: MassTransit/RabbitMQ flows from C# configs
- **Cross-Stack Integration Graph**: Links exchanges/queues across Node.js, C#, and infra configs

## Installation

1. Clone or download this repository

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:

**For Linux/Mac:**
```bash
pip install -r requirements.txt
```

**For Windows (recommended if you encounter gobject-2-0-0 errors):**
```bash
pip install -r requirements-windows.txt
```

**Note:** The main `requirements.txt` includes `weasyprint` which requires GTK/GObject libraries that can be problematic on Windows. If you encounter errors like "OSEerror gobeject-2-0-0 error" on Windows, use `requirements-windows.txt` instead, which uses `xhtml2pdf` (a Windows-compatible PDF generator).

4. (Optional) Set up environment variables:
```bash
cp .env.example .env  # On Windows: copy .env.example .env
# Edit .env with your API keys
```

## Configuration

Edit `config.yaml` to customize:
- Supported languages and file extensions
- LLM provider settings
- Documentation preferences
- Output settings

## Usage

### Web UI (Recommended)

Launch the modern web interface:

```bash
# Using the convenience script
./run_ui.sh

# Or directly with Streamlit
streamlit run app.py
```

The web UI provides:
- ðŸŽ¨ Modern, intuitive interface
- ðŸ“ Easy source selection (file upload, folder, or Git repo)
- ðŸ¤– LLM provider selection and configuration
- âš¡ Real-time generation progress
- ðŸ“„ Live preview of generated documentation
- ðŸ“¥ One-click download

The interface will open in your default browser at `http://localhost:8501`

### CLI Usage

### Basic Usage

Generate documentation from a single file:
```bash
python main.py --source path/to/file.java --type file
```

Generate documentation from a folder:
```bash
python main.py --source path/to/project --type folder
```

Generate documentation from a Git repository:
```bash
python main.py --source https://github.com/user/repo.git --type git
```

### Advanced Usage

Specify LLM provider:
```bash
python main.py --source ./src --type folder --provider ollama
```

Specify output file:
```bash
python main.py --source ./src --type folder --output ./my_docs.md
```

Use specific Git branch:
```bash
python main.py --source https://github.com/user/repo.git --type git --branch develop
```

Custom config file:
```bash
python main.py --source ./src --type folder --config custom_config.yaml
```

Verbose output:
```bash
python main.py --source ./src --type folder --verbose
```

Stream output for large repos:
```bash
python main.py --source ./src --type folder --stream --output ./docs/streamed_docs.md
```

Generate a configured domain profile:
```bash
python main.py --domain orders-service
```

Generate all domain profiles:
```bash
python main.py --all-domains
```

## LLM Provider Setup

### Ollama (Local LLM - Recommended for Privacy)

1. Install Ollama from https://ollama.ai
2. Pull a model:
```bash
ollama pull llama3.2
```
3. Use with the generator:
```bash
python main.py --source ./src --provider ollama
```

### OpenAI

1. Get API key from https://platform.openai.com
2. Set environment variable:
```bash
export OPENAI_API_KEY=your_key_here
```
3. Or add to `.env` file
4. Use:
```bash
python main.py --source ./src --provider openai
```

### Anthropic (Claude)

1. Get API key from https://console.anthropic.com
2. Set environment variable:
```bash
export ANTHROPIC_API_KEY=your_key_here
```
3. Use:
```bash
python main.py --source ./src --provider anthropic
```

### MCP (Model Context Protocol)

1. Set up MCP server
2. Configure in `config.yaml`:
```yaml
llm_providers:
  mcp:
    enabled: true
    server_url: http://localhost:8000
    model: your_model
```
3. Use:
```bash
python main.py --source ./src --provider mcp
```

## Project Structure

```
TechDocGen by IBMC/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ generator.py            # Main documentation generator
â”‚   â”œâ”€â”€ readers/                # Source code readers
â”‚   â”‚   â”œâ”€â”€ base_reader.py
â”‚   â”‚   â”œâ”€â”€ file_reader.py
â”‚   â”‚   â”œâ”€â”€ folder_reader.py
â”‚   â”‚   â””â”€â”€ git_reader.py
â”‚   â”œâ”€â”€ parsers/                # Language parsers
â”‚   â”‚   â”œâ”€â”€ base_parser.py
â”‚   â”‚   â”œâ”€â”€ java_parser.py
â”‚   â”‚   â”œâ”€â”€ csharp_parser.py
â”‚   â”‚   â”œâ”€â”€ vbnet_parser.py
â”‚   â”‚   â”œâ”€â”€ fsharp_parser.py
â”‚   â”‚   â””â”€â”€ php_parser.py
â”‚   â””â”€â”€ llm/                    # LLM integrations
â”‚       â”œâ”€â”€ base_llm.py
â”‚       â”œâ”€â”€ openai_llm.py
â”‚       â”œâ”€â”€ anthropic_llm.py
â”‚       â”œâ”€â”€ ollama_llm.py
â”‚       â”œâ”€â”€ mcp_llm.py
â”‚       â””â”€â”€ llm_factory.py
â”œâ”€â”€ main.py                     # CLI entry point
â”œâ”€â”€ config.yaml                 # Configuration file
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## Examples

### Example 1: Document a Java Project
```bash
python main.py --source ./my-java-project/src --type folder --provider ollama --output java_docs.md
```

### Example 2: Document a .NET Solution
```bash
python main.py --source ./MySolution --type folder --provider openai
```

### Example 3: Document a PHP Application from Git
```bash
python main.py --source https://github.com/user/php-app.git --type git --branch main --provider ollama
```

### Example 4: Document a Node.js/Angular Repo (JS/TS/HTML)
```bash
python main.py --source ./frontend --type folder --provider ollama --output frontend_docs.md
```

### Example 5: Document Infrastructure Configs (JSON/YAML)
```bash
python main.py --source ./infra --type folder --output infra_docs.md
```

### Example 6: Streaming + Chunking for Large Repo
```bash
python main.py --source ./monorepo --type folder --stream --output ./docs/monorepo.md
```
```yaml
documentation:
  streaming_mode: true
  chunk_size_chars: 200000
  chunk_overlap_chars: 1000
```

### Example 7: Domain Profile (Per-Service)
```bash
python main.py --domain orders-service
```
```yaml
domains:
  - name: "orders-service"
    type: "folder"
    path: "./services/orders"
    include_patterns:
      - "**/*.cs"
      - "**/*.json"
    exclude_patterns:
      - "**/bin/**"
      - "**/obj/**"
    streaming_mode: true
    output: "./docs/orders-service.md"
```

### Example 8: MassTransit/RabbitMQ Flow Extraction
```bash
python main.py --source ./services/orders --type folder --output orders_mq_docs.md
```
```csharp
cfg.ReceiveEndpoint("order-created", e =>
{
    e.Consumer<OrderCreatedConsumer>();
});
```

### Example 10: Cross-Stack Integration Graph (Node + Infra + C#)
```bash
python main.py --source ./examples --type folder --output ./docs/integration_graph.md
```
The generated docs include an "Integration Graph" section in Mermaid, linking:
- Node `amqplib` publishes/consumes
- Infra JSON/YAML queues/exchanges/bindings
- C# MassTransit consumers and send endpoints

### Example 9: Sample Projects in `examples/`

Use the included sample folders:
```bash
python main.py --source ./examples/angular-app --type folder --output ./docs/angular_example.md
python main.py --source ./examples/node-service --type folder --output ./docs/node_example.md
python main.py --source ./examples/csharp-masstransit --type folder --output ./docs/masstransit_example.md
python main.py --source ./examples/infra-config --type folder --output ./docs/infra_example.md
```

## Configuration Options

### Exclude Patterns

Add patterns to exclude files/directories in `config.yaml`:
```yaml
documentation:
  exclude_patterns:
    - "**/node_modules/**"
    - "**/vendor/**"
    - "**/bin/**"
```

### Include Patterns (Scope a Subset)

Only document matching files:
```yaml
documentation:
  include_patterns:
    - "**/*.cs"
    - "**/*.ts"
```

### File Size Limits

Set maximum file size:
```yaml
documentation:
  max_file_size_mb: 10
```

### Streaming + Chunking (Large Repos)

Enable streaming and chunking for large repositories:
```yaml
documentation:
  streaming_mode: true
  chunk_size_chars: 200000
  chunk_overlap_chars: 0
```

### Messaging Flow Extraction (MassTransit/RabbitMQ)

MassTransit queues/consumers/publishes/sends are automatically extracted from C# files. The docs include a "Messaging Flows" section when detected.

### Domain Profiles (Per-Service Docs)

Define per-service profiles in `config.yaml`:
```yaml
domains:
  - name: "orders-service"
    type: "folder"
    path: "./services/orders"
    include_patterns:
      - "**/*.cs"
      - "**/*.json"
    exclude_patterns:
      - "**/bin/**"
      - "**/obj/**"
    streaming_mode: true
    output: "./docs/orders-service.md"
```

### LLM Settings

Customize LLM behavior:
```yaml
llm_providers:
  ollama:
    model: llama3.2
    temperature: 0.3
    base_url: http://localhost:11434
```

## Troubleshooting

### Windows Installation Issues

**Error: "OSEerror gobeject-2-0-0 error" or similar GTK/GObject errors**

This occurs because `weasyprint` requires GTK/GObject libraries that are difficult to install on Windows. Solution:

1. Use the Windows-specific requirements file:
   ```bash
   pip install -r requirements-windows.txt
   ```

2. The application will automatically use `xhtml2pdf` instead of `weasyprint` on Windows, which doesn't require GTK dependencies.

3. If you still encounter issues, you can manually install:
   ```bash
   pip uninstall weasyprint
   pip install xhtml2pdf
   ```

### Ollama Connection Error
- Ensure Ollama is running: `ollama serve`
- Check the base URL in config matches your Ollama setup

### Git Repository Issues
- For private repos, ensure you have proper authentication
- Use SSH URLs for private repositories

### Large Projects
- Increase `max_file_size_mb` in config if needed
- Consider processing specific folders instead of entire projects

### PDF Generation Issues

**Error: "usedforsecurity is not a valid keyword argument for OpenSSL"**

This error occurs when using older versions of `xhtml2pdf` or `reportlab` with newer Python/OpenSSL versions. Solution:

1. Upgrade xhtml2pdf and reportlab:
   ```bash
   pip install --upgrade xhtml2pdf>=0.2.17 reportlab>=4.0.0
   ```

2. The code includes an automatic workaround, but if issues persist, reinstall:
   ```bash
   pip uninstall xhtml2pdf reportlab
   pip install xhtml2pdf>=0.2.17 reportlab>=4.0.0
   ```

**General PDF Generation Issues:**
- On Windows, the app automatically falls back to `xhtml2pdf` if `weasyprint` is unavailable
- If PDF generation fails, check that either `weasyprint` (Linux/Mac) or `xhtml2pdf` (Windows) is installed
- Ensure you're using xhtml2pdf version 0.2.17+ for OpenSSL compatibility

## License

This project is provided as-is for generating technical documentation.

## Contributing

Feel free to submit issues, fork the repository, and create pull requests for improvements.

